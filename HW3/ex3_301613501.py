# -*- coding: utf-8 -*-
"""Math_for_DS_and_SP_HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YsgIlgSIebhxdHA09ipuplNn3H6ny3SZ

# Mathematical Method for DS and PS
## HW 3
### Question 2
"""

import numpy as np
import matplotlib.pyplot as plt
import scipy as sc
from scipy import stats
from scipy.sparse.linalg import eigsh
from scipy.sparse.linalg import eigs
import cvxpy as cp
from ast import While
import scipy.linalg
import random

# Circular shifting
def cir_shf(shf, vec):
    idc = np.arange(len(vec))
    v = vec[np.mod(idc-shf, len(vec))]
    return v

# Mean square error (min)
def mse(v, v_hat, order=2):
    mse_min = float('inf')
    l_min = -1
    for l_i in range(len(v)):
        mse_cur = np.linalg.norm(v-cir_shf(l_i, v_hat), ord=order)
        if mse_cur < mse_min:
            mse_min = mse_cur
            l_min = l_i
    mse_min = mse_min * (np.linalg.norm(v) ** -1)
    return mse_min, l_min

# Observations generator
def gen_smp(dim, sample_num, dist, std, signal):
    l_arr = np.random.choice(np.arange(0, dim), size=sample_num, p=dist)
    eps_i = std * np.random.normal(0, 1, size=(dim, sample_num))
    smp = np.zeros_like(eps_i)
    for idx_n in range(sample_num):
        l_i = l_arr[idx_n]
        signal_shf = cir_shf(l_i, signal)
        smp_i = signal_shf + eps_i[:, idx_n].squeeze()
        smp[:, idx_n] = smp_i
    return smp

# Calculating the 1st and the 2nd moments of data y
def method_of_moments_order2(samples, std):
    n_samples = samples.shape[1]
    m1 = np.mean(samples, axis=1)
    m2 = (n_samples ** -1) * samples.dot(samples.T) - np.diag(std ** 2 * np.ones_like(m1))
    parameters = method_of_moments_order2_spec_clean(m1, m2)
    return parameters

# Algorithm 1 - estimatin the signal x
def method_of_moments_order2_spec_clean(m1, m2):
    dim = len(m1)
    fourier = scipy.linalg.dft(dim)
    fourier_inv = fourier.T.conj() * (dim ** -1)
    ### Normalize Fx ###
    # 1.1
    p_spc = abs(dim * np.diagonal(fourier.dot(m2).dot(fourier_inv)))
    # 1.2
    p_inv_hlf = p_spc ** -0.5
    dia_p_inv_hlf = np.diag(p_inv_hlf)
    # 1.3
    q = fourier_inv.dot(dia_p_inv_hlf).dot(fourier)
    # 1.4
    m2_wav = q.dot(m2).dot(q.conj())
    ### Extract eigenvector and rescale ###
    # 2.1
    eig_val, eig_vec = np.linalg.eig(m2_wav)
    eig_val_idc = abs(eig_val).argsort()
    eig_vec = eig_vec[:, eig_val_idc]
    eig_vec = eig_vec[:, -1]
    # 2.2
    eig_vec_wav = fourier_inv.dot((p_spc ** 0.5) * (fourier.dot(eig_vec)))
    # 2.3
    sig_hat = (m1.sum() / eig_vec_wav.sum()) * eig_vec_wav
    sig_hat = np.absolute(sig_hat) * np.sign(np.real(sig_hat))
    # 2.4
    rho_hat = np.linalg.inv(scipy.linalg.circulant(sig_hat)).dot(m1)
    return sig_hat, rho_hat

# Implementation of the Expectation-Maximization algorithm 
# from reference described in the document
class ExpMaxOptimizer:

    def __init__(
            self,
            samples,
            sigma,
            sig_hat=np.array([]),
            rho_hat=np.array([]),
            threshold=0.1,
            itr_max=100
    ):

        self.samples = samples
        self.weights = np.zeros_like(samples)
        self.sigma = sigma
        self.dim = samples.shape[0]
        self.n = samples.shape[1]
        # Initial guess for signal
        if len(sig_hat) == 0:
            self.sig_hat = samples.mean(axis=1)
        else:
            self.sig_hat = sig_hat
        # Initial guess for distribution
        if len(rho_hat) == 0:
            dist_unnorm = np.random.uniform(size=L)
            self.rho_hat = dist_unnorm / dist_unnorm.sum()
        else:
            self.rho_hat = rho_hat
        self.threshold = threshold
        self.itr_max = itr_max

    # E-Step of the EM
    def exp_step(self):
        # Update weights
        for idx_l in range(self.dim):
            shf_sig_hat = cir_shf(idx_l, self.sig_hat)
            shf_sig_hat_mat = np.tile(shf_sig_hat, (self.n, 1)).T
            diff_mat = shf_sig_hat_mat - self.samples
            gauss_arg = (diff_mat ** 2).sum(axis=0)
            self.weights[idx_l, :] = (
                self.rho_hat[idx_l]
                * np.exp(
                    -(0.5 * self.sigma ** -2)
                    * gauss_arg
                    )
                )
        # Normalize weights
        nrm_vec = self.weights.sum(axis=0) ** -1
        nrm_mat = np.tile(nrm_vec, (self.dim, 1))
        self.weights = self.weights * nrm_mat
        # update signal value
        self.sig_hat = np.zeros_like(self.sig_hat)
        for idx_l in range(self.dim):
            weights_vec = self.weights[idx_l, :]
            shf_smp = cir_shf(-idx_l, self.samples)
            self.sig_hat += weights_vec.dot(shf_smp.T) * (self.n ** -1)
    # M-Step of the EM
    def max_step(self):
        dist_unnorm = self.weights.sum(axis=1)
        self.rho_hat = dist_unnorm * (dist_unnorm.sum() ** -1)
    
    # Iterative optimizations
    def optimize(self):
        # Run iterations
        for itr_idx in range(self.itr_max):
            # temporary variable
            sig_hat_tmp = self.sig_hat
            # Expectation step
            self.exp_step()
            # Maximization step
            self.max_step()
            if np.linalg.norm(sig_hat_tmp - self.sig_hat) < self.threshold:
                break
        return self.sig_hat, self.rho_hat

# Initialize the random number generator
rnd_max = 2 ** 31
rnd_seed = random.randint(0, rnd_max)
np.random.seed(rnd_seed)

# General Parameters
n = int(1e4)    # Num of Observations
L = 15          # Lenght of the signal x
num_exp = 10     # Num of experiments
num_stp = 100   # Num of steps

# Generate a signal x for all 
x = np.random.normal(0, L ** -0.5, L)

# Create a distribution rho for all
rho_unnorm = np.random.uniform(size=L)
rho = rho_unnorm / rho_unnorm.sum()

"""### Question 2.a"""

# Question 2 - a

sig_str = 0.1   # Sigma start value
sig_end = 10    # Sigma end value
sig_stp = 10
rnd_exp_seed_arr = np.random.randint(0, rnd_max, num_exp)
sigma_arr = np.linspace(sig_str, sig_end, sig_stp)
#sigma_arr = np.logspace(np.log10(sig_str), np.log10(sig_end), sig_stp)

# Vectors initialization
MSE_MetMom_1 = np.zeros_like(sigma_arr)
MSE_ExpMax_1 = np.zeros_like(sigma_arr)

for idx_stp in range(sig_stp):
    sigma = sigma_arr[idx_stp]
    for idx_exp in range(num_exp):
        # Use the same seed
        rnd_exp_seed = rnd_exp_seed_arr[idx_exp]
        np.random.seed(rnd_exp_seed)
        # Generate noisy data
        y = gen_smp(L, n, rho, sigma, x)
        # Method of Moments
        x_hat_MetMom = method_of_moments_order2(y, sigma)[0]
        MSE_MetMom_1[idx_stp] += (mse(x, x_hat_MetMom, order=2)[0] / num_exp)
        # Expectation Maximization
        ExpMax_opt = ExpMaxOptimizer(y, sigma)
        x_hat_ExpMax = ExpMax_opt.optimize()[0]
        MSE_ExpMax_1[idx_stp] += (mse(x, x_hat_ExpMax, order=2)[0] / num_exp)

# Logaritmic Plot
plt.figure(1)
plt.loglog(sigma_arr, MSE_MetMom_1, 'r.-', label='Method of Moments')
plt.loglog(sigma_arr, MSE_ExpMax_1, 'b.-',label='Expectation Maximization')
plt.legend()
plt.title('Average Error as Function of Sigma')
plt.ylabel('MSE Average')
plt.xlabel('sigma')
plt.grid()

plt.show()

# Plot
plt.figure(2)
plt.plot(sigma_arr, MSE_MetMom_1, 'r.-', label='Method of Moments')
plt.plot(sigma_arr, MSE_ExpMax_1, 'b.-',label='Expectation Maximization')
plt.legend()
plt.title('Average Error as Function of Sigma')
plt.ylabel('MSE Average')
plt.xlabel('sigma')
plt.grid()

plt.show()

"""### Question 2.b"""

# Question 2 - b

n_str = int(1e2)
n_end = int(1e5)
# n_arr = np.linspace(n_str, n_end, num_stp)
n_arr = np.logspace(np.log10(n_str), np.log10(n_end), num_stp).astype(dtype=int)
sigma = 1

MSE_MetMom_2 = np.zeros_like(n_arr)
MSE_ExpMax_2 = np.zeros_like(n_arr)

for idx_stp in range(num_stp):
    n = n_arr[idx_stp]
    for idx_exp in range(num_exp):
        # Use the same seed
        rnd_exp_seed = rnd_exp_seed_arr[idx_exp]
        np.random.seed(rnd_exp_seed)
        # Generate noisy data
        y = gen_smp(L, n, rho, sigma, x)
        # Method of Moments
        x_hat_MetMom = method_of_moments_order2(y, sigma)[0]
        MSE_MetMom_2[idx_stp] += (mse(x, x_hat_MetMom, order=2)[0] / num_exp)
        # Expectation Maximization
        ExpMax_opt = ExpMaxOptimizer(y, sigma)
        x_hat_ExpMax = ExpMax_opt.optimize()[0]
        MSE_ExpMax_2[idx_stp] += (mse(x, x_hat_ExpMax, order=2)[0] / num_exp)

# Logaritmic Plot
plt.figure(3)
plt.loglog(n_arr, MSE_MetMom_2, 'sr', markerfacecolor='none', label='Method of Moments')
plt.loglog(n_arr, MSE_ExpMax_2, '+b', label='Expectation Maximization')
plt.legend()
plt.title('Average Error as Function of Sigma')
plt.ylabel('MSE Average')
plt.xlabel('n observations')
plt.grid()

for i in range(len(n_arr)):
    plt.vlines(n_arr[i], 0, MSE_MetMom_2[i], 'r', linestyle='dotted', linewidth=0.2)

for j in range(len(n_arr)):
    plt.vlines(n_arr[j], 0, MSE_ExpMax_2[j], 'b', linestyle='dotted', linewidth=0.2)

plt.show()

# Plot
plt.figure(4)
plt.semilogx(n_arr, MSE_MetMom_2, 'sr', markerfacecolor='none', label='Method of Moments')
plt.semilogx(n_arr, MSE_ExpMax_2, '+b', label='Expectation Maximization')
plt.legend()
plt.title('Average Error as Function of Sigma')
plt.ylabel('MSE Average')
plt.xlabel('n observations')
plt.grid()
for i in range(len(n_arr)):
    plt.vlines(n_arr[i], 0, MSE_MetMom_2[i], 'r', linestyle='dotted', linewidth=0.5)

for j in range(len(n_arr)):
    plt.vlines(n_arr[j], 0, MSE_ExpMax_2[j], 'b', linestyle='dotted', linewidth=0.5)

plt.show()

